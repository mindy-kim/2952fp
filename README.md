# Robustness through Forgetting: Unlearning Adversarial Contexts in Transformers

Transformer-based models have revolutionized the natural language processing scene, particularly with their unique in-context learning capabilities that allow them to adapt to novel tasks. However, as these models seem to make bounds and leaps in terms of improved performance, there is a growing push to ensure these models remain safe for users, with problems from privacy violations to copyright issues continuing to arise through nothing more than an unsafe or sometimes even harmless prompt. This paper explores the efficacy of machine unlearning, an efficient technique used to train a model when certain data needs to be 'forgotten', to improve the robustness of transformer-based models against harmful prompts, specifically hijacking attacks. By focusing on a simplistic version of the data and these complex transformers, we identify patterns and dependencies that drive model behavior and illuminate the efficacy of utilizing machine unlearning to robustly train transformers and prevent harmful responses. This research is particularly timely, as the widespread adoption of transformers in critical applications demands a deeper understanding of their capabilities and limitations.
